# **IBM Data Analyst Professional Certificate**

# _Introduction to Data Analytics_:

## 1st Week - Intro to Data Analytics:

- Data Analytics Career Path:
  - **Data Analyst Specialist Roles:**
    - Pathes:
      - Associate | Junior Data Analyst
      - Data Analyst
      - Senior Data Analyst
      - Lead Data Analyst
      - Principal Data Analyst
    - Boundaries to move next level:
      - Years of experience
      - Nature of experience for the next level
      - Vary depending on:
        - Industry
        - Size of organization
        - How big your team is
  - **Domain Specialist Roles:**
    - Need a Specialization in Specific Domain:
      - Healthcare
      - Finance
      - Social Media
      - Digital Marketing
    - May or may not be technically skilled people
    - Examples:
      - HR Analyst
      - Marketing Analyst
      - Sales Analyst
      - Healthcare Analyst
      - Social Media Analyst
  - **Analytics-Enabled Job Roles:**
    - Examples:
      - Project Managers
      - Marketing Managers
      - HR Managers
  - **Other Data Professionals:**
    - Examples:
      - Data Engineers
      - Data Scientists
      - Business Analysts
      - Business Intelligence Analysts

- Branching into other data professions:
  - Data Science
  - Data Engineering
  - Business Analytics
  - Business Intelligence Analytics
- Data-Driven Decision Maker
- Fundamentals of Data Analysis:
  - Gather, Clean, Analyze, and Mine Data
  - Interpret Results
  - Reports Findings
- Modern Data Ecosystem:
  - Infrastructure
  - DataOps
  - Consumption &amp; Apps
  - **Emerging Technologies shaping the modern data ecosystem:**
    - Cloud Technologies
    - Machine Learning
    - Big Data:
      - V&#39;s Of Big Data:
        - **Velocity:**
          - The speed at which data accumulates
        - **Volume:**
          - Increase in Data Source
          - High Resolution Sensors
          - Scalable Infrastructure
        - **Variety:**
          - Diversity of Data:
            - Type:
              - Structured
              - Semi-Structured
              - UnStructured
            - Sources:
              - Machines
              - Peoples
              - Processes
        - **Veracity:**
          - Quality and Origin of Data
          - **Attributes** :
            - Consistency
            - Completeness
            - Integrity
            - Ambiguity
          - **Drivers:**
            - Cost
            - Need for Traceability
        - **Value:**
          - Turn Data into Value
      - **Big Data Processing Tools:**
        - Apache Spark:
          - A Distributed Analytics framework for complex, real-time data analytics
        - Apache Hadoop:
          - Collection of tools that provides:
            - Distributed Storage
            - Processing of Big Data
          - **Node** is a Single **Computer**
          - **Collection** of **Nodes** forms a **Cluster**
          - Hadoop Distributed File System
        - Apache Hive:
          - A Data **Warehouse** for data **query** and **analysis**
          - Build **on Top of Hadoop**
- Data &amp; Data Sources:
  - Data:
    - **Structured Data:**
      - Quantitative
      - g: Data Warehouses, Relational Databases
      - Several Predetermined Formats
      - Examples:
        - Can be displayed in Relational Databases
        - Numbers,dates,strings
        - SQL Databases
        - Online Transaction Processing Systems (OLTP) that focus on business transactions
        - Spreadsheets
        - Online Forms
        - Sensors GPS
        - Radio Frequency Identification (RFID)
        - Network and Web Server Logs
    - **Semi-Structured Data:**
      - Lacks a fixed or rigid schema
      - Tags ,Elements, MetaData which is used to group data and organize it in a hierarchy
      - Examples:
        - **Email** :
          - Structured Data:
            - Name of the Sender and Recipient
          - UnStructured Data:
            - The Content
        - **XML and other Markup Langauges**
        - **Binary Executables**
        - **TCP / IP Packets**
        - **Zipped Files**
        - **Integration of Data**
    - **Unstructured Data:**
      - Qualitative
      - Data is Complex and mostly qualitative information that is **impossible to reduce to Rows and Columns**
      - g: Data Lakes, Non-Relational Databases
      - A huge array of formats
      - Examples:
        - Images,Audios,Videos,word-processing-files,emails,spreadsheets
        - Can not be displayed in rows,columns in Relational Database
        - **Web Pages**
        - **Social Media Feeds**
        - **Images in Varied File Formats**
        - **Video and Audio Files**
        - **Documents and PDF Files**
        - **PowerPoint Presentations**
        - **Media Logs**
        - **Surveys**
      - Ways to Store:
        - Files and Docs:
          - Manual Analysis
        - NOSQL:
          - Analysis Tools
  - Data Formats:
    - Relational Database
    - Non-Relational Database
    - APIs
    - Web Services
    - Data Streams
    - Social Platforms
    - Sensor Devices
  - Languages Available in Data Analyst Ecosystem:
    - **Query Languages:**
      - SQL:
        - Querying and Manipulating Data
        - CRUD Operations in a database
        - Write Store Procedures:
          - Write a set of instructions and call them for later user
        - Runs on an interpreter system
    - **Programming Languages:**
      - Python **:**
        - Supports multiple programming paradigms - Object Oriented, Imperative, Functional, and procedural
        - **Libraries** :
          - Pandas:
            - Data Cleaning and Analysis
          - Numpy and Scipy:
            - Statistical Analysis
          - Beautiful Soup, Scrapy:
            - Web Scraping
          - Matplotlib and Seaborn:
            - Data Visualization
      - R:
        - Facilitates handling of structured and unstructured data
    - **Shell and Scripting Languages:**
      - For Repetitive and Time-Consuming Operational Tasks
      - Examples:
        - Unix/Linux Shell:
          - Written Program for the UNIX shell. It is a series of **UNIX commands** written in **a plain text file** to accomplish a specific task
          - **Operations:**
            - File Manipulation
            - Program Execution
            - **System Administration Tasks:**
              - Disk backups
              - Evaluating System Logs
            - Installation scripts for complex programs
            - Executing routine backups
            - **Running batches**
        - PowerShell:
          - Cross-Platform Automation Tool, Configuration Framework by Microsoft
          - Optimized for working with structured data formats:
            - JSON
            - CSV
            - XML
            - REST Apis
            - Websites
            - Office Applications
          - Components:
            - Command Line Shell + Scripting Language
            - Object-Based:
              - Filter / Measure / Group / Compare Objects
          - Used In:
            - Data Mining:
              - The Process of Extracting Knowledge from data
              - Goals to find:
                - Correlations
                - Patterns
                - Variations
                - Understanding Trends
                - Predict Probabilities
              - Tools for Data Mining:
                - Spreadsheets:
                - Excel Add-ins
                - R-Language
                - Python
              - _ **Classification:** _
                - Classifying **Attributes** into **target categories**
              - _ **Clustering:** _
                - Involves grouping data into **clusters** so they can be treated as **groups**
              - _ **Anomaly or Outlier Detection:** _
                - Finding Patterns in Data that are not **normal** or **unexpected**
              - _ **Association Rule Mining:** _
                - Establishing a relationship between two data events
              - _ **Sequential Patterns** _
              - _ **Affinity Group** _
              - _ **Decision trees:** _
                - _Building a classification model that helps out building a clear understanding of relationship between input and output_
              - _ **Regression** _:
                - Identifying the nature of relationship between two variables which could be **causal** or **correlational**
              - Applications:
                - Profiling Customer Behaviors
                - Tracking Unusual Behaviors in Customer transactions at financial institutions
                -
            - Building GUIs
            - Creating Charts, Dashboard, Interactive Reports
  - Data Sources | Repositories:
    - Databases | Data Warehouses | Data Marts | Data Lakes | Big Data Stores
    - **Type of Data** &amp; **File Formats** &amp; **Sources of Data** influence the type of data repositories that you could use
    - Data Repository is a general term used to refer the data that has been **collected** , **organized** , and **isolated:**
      - Used in **Business Operations**
      - Used in **Mining for Reporting and Data Analysis**
    - Types of Data Repositories ↓
    - **Data Warehouse:**
      - Consolidate data in one Place through ETL Process
      - A Single Source of Truth:
        - Transformed from Raw Data to High Quality Data that will be stored later in warehouse and that process is done with the help of ETL:
          - Process of how Raw Data is converted into analysis-ready data
          - It&#39;s common to see the terms **ETL** and **Data**** Pipelines **used** interchangeably**
          - Data Pipelines:
            - Entire Journey of moving data from one system to another, Including ETL Process which is a subset of Data Pipeline Entire journey
            - Can be used for batch and streaming data
          - **Data Wrangling:**
            - Extract:
              - Ways:
                - **Batch-Processing** - Large Chunks of data moved from source to destination at scheduled intervals Tools: **Slitch** &amp; **Blendo**
                - **Stream Processing** - Data Pulled in **real-time** from source, transformed in **transit** , and **loaded** into **data transit** Tools : Apache Samza,Storm,Kafka
              - From a source
              - Passed to Staging Area Before sent directly to warehouse
              - Handle Structured and UnStructured Data
              - **Logical Extraction**
              - **Physical Extraction:**
                - Online Extraction
                - Physical Extraction
            - Transform:
              - Row Data to Data used for Analysis
              - Data Cleaning and Organization
                - Single System Format
                - Improving Data Quality
          - Load:
            - **Steps:**
              - **Initial Loading:**
                - **Populating**** all of the data** in the repository
              - **Incremental Loading:**
                - Applying **updates** and **modifications** Periodically
              - **Full Refresh:**
                - **Erasing** contents from one of tables and **reloading**** refresh data**
            - Data Sent to Warehouse
            - **Load Verifications** Checks for:
              - **Missing or null Values**
              - **Server Performance**
              - **Load Failures**
            - Batch Loading Operating System:
              - Groups jobs that perform similar type of functions. These groups are called as batch and are executed at the same time
            - Incremental Loading
            - Full Loading
        - Structuring **All the BEST QUALITY DATA in ONE PLACE**
      - One Single Place - One Single Source of Information
      - Where Companies store Valuable Data Assets are stored:
        - Customer Data
        - Sales Data
        - Employees Data
        - …etc
    - Features of Data Warehouse:
      - Subject Oriented
      - Integrated
      - Time-Variant
      - Non-Volatile
    - Data Lake vs Data WareHouse vs Data Marts:
      - **Data Lake:**
        - Contains raw, structured, unstructured data in a quick way to clean and organize later
        - Data Lake is a **pool** of **raw** data where each data element is given **a unique identifier** and is **tagged** with **metatags** for further use
        - **Data Warehouse:**
          - A large collection of organized and clean business data help organization make decisions
          - Then:
            - Users
              - Business Stakeholders
              - Apps
              - Programmers Analysts
              - Data Science Use Cases
            - Through:
              - Interfaces
              - APIs
              - Applications
          - **Data Marts:**
          - Subset of a data warehouse that&#39;s more specific to a particular business domain e.g Finance data mart
- Data Professionals Process:
  - 1st Role:
    - Data Engineer:
      - **Responsibilities:**
        - Develop and Maintain Data Architectures
        - Make Data available for:
          - Business Operations
          - Analysis
        - Make the ETL Process (as we explained earlier)
      - **Requirements to Be Data Engineer:**
        - Good Knowledge of Programming
        - Sound Knowledge of Systems and Technology Architecture
        - In-Depth understanding of Relational and Non-Relational Databases
  - 2nd Role:
    - Data Analyst (Science + Art):
      - **Responsibilities:**
        - **Technical Skills:**
          - **Inspect and clean data** for deriving insights
          - Identify **Correlations** , Find **Patterns** , and apply **statistical methods** to analyze and mine data
          - **Visualize Data** to interpret and present the findings of data analysis
          - Are the people who **answer the questions**
          - Translates data and numbers into plain language on which organization depend to take decisions
        - **Functional Skills:**
          - Proficiency in Statistics:
            - Analyze Data,
            - Validate Analysis,
            - Identify Fallacies and Logical Errors
          - Analytical Skills:
            - Research and interpret data, theorize,make forecasts
          - Problem-Solving Skills:
          - Probing Skills:
            - Identify and define the problem statement and desired outcome
          - Data Visualization Skills
          - Project Management Skills
        - **Soft Skills:**
          - Collaboration work with Cross-Functional Teams
          - Written and Verbal Communication
          - Story Telling
          - Gather Support and buy-in for your work
          - Curiosity:
            - New Questiosn
            - Challenging your own hypothesis
          - Intuition:
            - Future based on pattern recognition and past experiences
      - **Requirements to Be a Data Analyst:**
        - Good Knowledge of:
          - Spreadsheets
          - Writing Queries
          - Statistical tools to create charts and dashboards
        - Programming Skills
        - Strong Analytical and story-telling Skills
      - **Applications of Data Analytics in today&#39;s world:**
        - Used by **companies** to identify what **information**** consumers **want them to** share**
        - Used by **people** with **diabetes** monitoring sugar levels
        - Sales **Pipeline analysis**
        - Financial **Reporting**
        - Airlines,Banking,...etc
        - Use of **Sentiment analysis** of tweets and stories to inform investment decisions
        - Use of **satellite imagery data** to track the **development** of **industrial activities**
  - 3rd Role:
    - Data Scientist:
      - **Responsibilities** :
        - Analyze data for actionable insights
        - Create Predictive Models using Machine and Deep Learning
        - &quot;More nearer to **answering future questions**&quot;
      - **Requirements to Be a Data Scientist:**
        - Knowledge of Math and Statistics
        - Understanding Programming Languages,databases,building data models
        - Domain Knowledge
  - 4th Role:
    - Business Analyst and BI Analyst:
      - **Responsibilities** :
        - Data Analyst Role + Data Scientist Role
        - Focus more on market forces and external influences that shape their business
  - Connection between Roles:
    - **Data Engineer:**
      - Convert Raw data into usable data
      - **Data Analytics:**
        - Use this data to generate insights
        - **Data Scientist:**
          - Use Data Analytics + Data Engineer to predict the future using data from past
          - **Business Analyst and BI Analyst:**
            - Use these insights and predictions to drive decisions that benefit and grow their business
- Different Types of Data Analysis:
  - **Descriptive Analytics:**
    - **What** Happened?
      - Provides Insights into past events
  - **Diagnostic Analytics:**
    - **Why** did it happen?
      - Insights from Descriptive Analytics to dig deeper to find the cause of outcome
  - **Predictive Analytics:**
    - What **might happen** in Future?
      - Leverages historical data and trends to predict future outcomes
      - All **Predictions** are **probabilistic** in Nature
  - **Prescriptive Analytics:**
    - What should be done about it?
    - Ex: Self-Driving Cars:
      - Analyze environments to make decisions regarding speed,chanings lanes,which route to take
- The Data Analysis Process:
  - **Understand the Problem and Desired Result:**
    - Where you&#39;re are, and where you want to be
    - Working Hypotheses:
      - Initial Hypotheses
  - **Setting a clear metric:**
    - What will be measured and how it will be measured
  - **Gathering Data:**
    - Identify the datasets that I&#39;m going to isolate and analyze to validate or refute my hypyothesis
  - **Cleaning Data:**
    - Fix Quality Issues in the Data that affect the accuracy of analysis , data standardization
    - Clean data from Missing,Incomplete,outliers values
  - **Analyzing and Mining Data:**
    - Extracting,Analyzing,and Manipulating data from different perspective to understand trends,identify correlations,find patterns and variations
  - **Interpreting Results:**
    - Circumstances under which your analysis may not hold true
  - **Presenting your findings:**
    - Initial Questions:
      - **Who** is my **audience**?
      - **What** is **important** to them?
      - **What** will **help** them **trust** me?
    - Notes:
      - **Data** you collected is like **a black box** for **audience**
      - **IT DOES NOT MATTER WHAT INFORMATION YOU HAVE IF YOU CAN&#39;T COMMUNICATE IT EFFECTIVELY TO YOUR AUDIENCE**
    - Data Visualization:
      - Types:
        - **Bar Charts** :
          - Comparing **Related Data Sets or parts** of a whole
        - **Column Charts:**
          - Compare Values **side-by-side**. You can use them quite effectively to show **change over time**
        - **Pie Charts:**
          - Breakdown of an entity into its sub-parts
        - **Line Charts:**
          - Display **trends**. They are great for showing how a data value is **changing** in relation to **a continuous variable**
        - **Dashboards:**
          - Organize and Display **Reports** and **Visualizations** from **multiple data sources** into a single **GUI**
          - Provide big, detailed picture by drilling into the next level of info for each param
      - Tools:
        - Spreadsheets
        - Jupyter Notebook and Python Libraries
        - Tableau
        - Microsoft Power BI
        - IBM Cognos Analytics
        - R-Studio and R-Shiny

## 2nd Week - The Data Ecosystem &amp; Languages for Data Pros:

- A Data Analyst&#39;s Ecosystem includes the Infrastructure, Software, Tools, Frameworks, And Processes used to:
  - Gather Data
  - Clean Data
  - Mine Data
  - Visualize Data
- Data Comprises:
  - Facts, Observations, Perceptions
  - Number, Characters, Symbols
  - Images
- Standard File Formats:
  - Delimited Text File Formats e.g CSV:
    - Files used to store data as text each value is separated by a delimiter
    - Delimiter:
      - A **sequence** of one or more characters for specifying the **boundary** between **independent entities or values**
      - Common Delimiters:
        - Comma
        - Tab
        - Colon
        - Vertical Bar
        - Space
      - Common used Types:
        - Comma-Separated Values:
          - **The First Row** works as a **Column Header** where each Column can have a different type of data
        - Tab-Separated Values
      -
  - Microsoft Excel Open .XML SpreadSheet or .XLSX:
    - Open File Format, Accessible to most other applications
    - Is **a Secure File Format** as it **cannot save malicious code**
  - Extensible Markup Language .XML:
    - **Markup language** with set **rules** for **encoding** data
    - **Self-Descriptive Language** for **sending info** over the internet
    - **Does not** use **predefined** tags like **.HTML does**
    - Platform Independent
  - Portable Document Format or .PDF:
    - Can also be used to **fill** in data for **forms**
  - JavaScript Object Notation or .JSON:
    - **Text-Based** open Standard Designed for **Transmitting**** structured ****data** over the web
    - Can be **read** in **any programming language**
    - Considered as one of the best tools for **sharing data**
- Common Sources of Data:
  - Databases:
    - Relational Databases:
      - Tabular Format with Rows and Columns
      - Optimized for data operations and querying
      - Using SQL as the standard querying language for Relational Databases
      - **Support &amp; Open-Source Classifications:**
        - Open-Source with **internal support**
        - Open-Source with **commercial support**
        - **Commercial closed-source**
        - Cloud-Based Relational Databases, or Database-as-a-Service:
          - Google SQl
          - IBM DB2 on CLoud
          - ORACLE cloud
          - Azure SQL
      - **Create Meaningful Information** by joining tables
      - **Minimize Data Redundancy**
      - **Backup and disaster recovery**
      - **ACID Compliant** Ensures:
        - Atomic
        - Consistency
        - Isolation
        - Durability
      - Limitations of RDBMS:
        - Does not work well with semi-structured and unstructured data
        - Migration (need to be identical)
      - Examples:
        - Microsoft SQL Server
        - Oracle
        - MySQL
        - IBM DB2
    - Non-Relational Databases:
      - Synonyms:
        - NoSQL
        - Not Only SQL
        - Non SQL
      - Allows data to be stored in a schema-less or free-form fashion
      - Pros:
        - Its ability to handle large volumens of structured, semi-structured, and unstructured data
        - Run as a distributed system scaled across multiple data centers
        -
      - **Types:**
        - **Key-Value Store:**
          - Data is stored as a collection of key-value pairs
          - Usages:
            - **User Session Data**
            - **User Preferences**
            - **Real-Time Recommendations**
            - Targeted Advertising
            - **In-Memory Data Caching**
          - Not a Great fit if you want to:
            - **Query** data on a **specific** data value
            - Need **Relationships** between **data**** values**
            - Need **multiple unique keys**
        - **Document Based:**
          - Store each record and its associated data within a single document
          - Usages:
            - E-Commerce Platforms
            - Medical Records Storage
            - CRM Platforms
            - Analytical Platforms
          - Not a Great fit if you want to:
            - Run Complex Search Queries
            - Perform multi-operation transactions
          - Examples:
            - Mongo DB
            - Document DB
            - Couch DB
            - Cloudant
        - **Column Based:**
          - Data is stored in cells grouped as columns of data instead of rows
          - **A logical grouping of columns** is referred to as **a column family**
        - **Graph Based:**
          - Useful for **visualizing** , **analyzing** , and finding **connections** between **different** pieces of data
          - Usages:
            - Social Networks
            - Product Recommendations
            - Network Diagrams
            - Fraud Detection
            - Access Management
          - Not a Great fit if you want to:
            - Process High Volumes of transactions
      -
    - **NOTE:**
      - Even though a **database** and **DBMS** mean different thing the terms are often used **interchangeably**
    - Factors governing choice of database:
      - Data Type
      - Data Structure
      - Querying Mechanisms
      - Latency Requirements
      - Transaction speeds
      - Intended use of data
  - Flat Files and XML Datasets:
    - Visibility:
      - Public
      - Privat
    - Flat Files:
      - Store data in plain Text Format
      - Each line, or row, is one record
      - Each value is separated by a delimiter
      - All of the data in a flat files maps to a single table
    -
  - APIs and Web Services:
    - Typically listen for incoming requests:
      - Web Requests
      - Network Request
    - Return Data in:
      - Plain Text
      - HTML
      - XML
      - JSON
      - Media Files
    - **Popular Examples of APIs:**
      - **Twitter and Facebook APIs** :
        - Customer Sentiment Analysis
        - Opinion Mining
        - **Purpose:**
          - To summarize the amount of appreciation or criticism on a given subject
      - **Stock Market APIs:**
        - For Trading and Analysis
      - **Data Lookup and Validation APIs:**
        - Cleaning and Correlating Data
  - Web Scraping:
    - Extract Data from Unstructured Sources
    - Synonyms:
      - Screen Scraping
      - Web Harvesting
      - Web Data Extraction
    - **Popular uses:**
      - Collecting product Details from retailers, Manufacturers, and E-Commerce Websites
      - Generating sales leads through public data sources
      - Extracting data from posts and authors on various formats
      - **Collecting**** training **and** testing **datasets for** machine learning models**
    - **Popular Web Scraping Tools:**
      - Beautiful Soup
      - Scrapy
      - Pandas
      - Selenium
  - Data Streams and Feeds:
    - Sources:
      - Instruments
      - IOT Devices
      - Applications
      - GPS Data
      - Websites
      - Social Media Posts
    - Technologies to process data streams:
      - Kafka
      - Apache Spark Streaming
      - Apache Storm
  - RSS:
    - Really Simple Syndication Feeds
    - Capturing updated data from online forums and news sites where data is refreshed on an ongoing basis

## 3rd Week - Gathering &amp; Wrangling Data:

- **Gathering Data:**
  - Determine the Data you want to collect:
    - The **specific Data** you need
    - The possible **sources** for this data
  - Define a Plan for Collecting Data:
    - Establish a **Timeframe** for collecting data
    - **How much data** is **sufficient** for a credible **analysis**?
    - Define **dependencies** , **risks** , and **mitigation** plan
  - Determine your data collection methods:
    - The methods depend on:
      - Sources of Data:
        - Primary Data:
          - Data from Organizations&#39;CRM,HR,Workflow applications
          - Data **you gather** through surveys,interviews,..etc
        - Secondary Data:
          - Data Retrieved from **Existing** sources
          - Examples:
            - External Databases
            - Research articles,Publications,Training Material
            - External Conducted Survetys,..etc
        - Third-Party Data:
          - Data **Purchased** from **aggregators** who collect data from **various sources** and combine it into **comprehensive** datasets for purpose of **selling** the **data**
      - Types of Data
      - Timeframe over which you need the data
      - Volume of data
    - Basic Key Considerations:
      - Data Quality:
        - Conditions:
          - Free of Errors
          - Accurate
          - Complete
          - Relevant
          - Accessible
        - Examples of Bad Quality Data:
        - Missing Data
        - Inconsistent Data
        - Incorrect Data
        - **This will lead to:**
          - **False Conclusions**
          - **Ineffective Decisions**
      - Data Governance:
        - Data Security
        - Data Regulation
        - Data Compliances
- **Data Wrangling | Data Munging:**
  - Iterative process for converting Raw Data to Data ready for analysis
  - Processes:
    - **Discovery** Phase
    - **Transformation** Phase:
      - **Structuring:**
        - **Joins &amp; Unios** (common Structural Transformations)
      - Normalizing and Denormalizing Data:
        - **Normalization** :
          - Processes:
            - Cleaning **Unused Data**
            - Reducing Redundancy
            - Reducing Inconsistency
          - Example:
            - Data Coming from Transaction Systems must be normalized following ACID Principles in the First Place
        - **Denormalization** :
          - Processes:
            - **Combining** Data from **Multiple** Tables into a **Single** Table for **faster querying** of data for reports and analysis
          - Examples:
            - Normalized Data coming from Transactional Systems are typically **denormalized** before running queries for reporting and analysis
      - **Cleaning** Data:
        - Missing Data
        - Null Values
        - Duplicate Data
        - Irrelevant Data
        - Data Type Conversion
        - Outliers
      - **Enriching** Data:
        - Adding Data Points that make your analysis more meaningful
    - **Validation** Phase:
      - Quality, Security of Data
    - **Publishing** Phase
  - Tools for Data Wrangling:
    - Excel Power Query | Spreadsheets
    - OpenRefine
    - Google DataPrep
    - Watson Studio Refinery
    - Trifacta Wrangler
    - Python
    - R

## 4th Week - Overview of Statistical Analysis:

- Statistics:
  - A branch of Math dealing with:
    - **Collection**
    - **Analysis**
    - **Interpretation**
    - **Presentation of Quantitative Data**
  - Types of Statistics:
    - Descriptive Statistics:
      - **Summarizing** Information about the Sample
      - Common Measures:
        - Central Tendency:
          - Mean
          - Median
          - Mode
        - Dispersion:
          - Variance
          - Standard Deviation:
            - How **Tightly** your data is **Clustered** around the **mean**
          - Range
        - Skewness:
          - Symmetrical
          - Skewed Left
          - Skewed Right
    - Inferential Statistics:
      - Making **Inferences** or **Generalizations** about the broader population
      - Common methodologies of Inferential Statistics:
        - **Hypothesis Testing**
        - **Confidence Intervals**
        - **Regression Analysis**

##